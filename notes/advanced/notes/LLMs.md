# LLMs 
> Large Language Models (大语言模型)

## Reference

- [langchain](https://github.com/hwchase17/langchain)
    > ⚡ Building applications with LLMs through composability ⚡

- [LLaMA](https://github.com/facebookresearch/llama)
    > Inference code for LLaMA models
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
    > Port of Facebook's LLaMA model in C/C++
- [Stanford Alpaca](https//github.com/tatsu-lab/stanford_alpaca)
    > Code and documentation to train Stanford's Alpaca models, and generate the data.

- [guidance](https://github.com/microsoft/guidance)
    > A guidance language for controlling large language models.    
- [GPT4All](https://github.com/nomic-ai/gpt4all) : [Official](https://gpt4all.io)
    > gpt4all: an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue
- [ChatLLaMa](https://github.com/juncongmoo/chatllama)
    > ChatLLaMA 📢 Open source implementation for LLaMA-based ChatGPT runnable in a single GPU. 15x faster training process than ChatGPT
- [pyllama](https://github.com/juncongmoo/pyllama)
    > 🦙 LLaMA - Run LLM in A Single 4GB GPU
- [minichatgpt](https://github.com/juncongmoo/minichatgpt) 
    > 🔥 To Train ChatGPT In 5 Minutes with ColossalAI
- [privateGPT](https://github.com/imartinez/privateGPT)
    > Interact privately with your documents using the power of GPT, 100% privately, no data leaks
- [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)
    > 中文LLaMA&Alpaca大语言模型+本地CPU/GPU部署 (Chinese LLaMA & Alpaca LLMs)
- [KoAlpaca](https://github.com/Beomi/KoAlpaca)
    > KoAlpaca: Korean Alpaca Model based on Stanford Alpaca (feat. LLAMA and Polyglot-ko)

## Concept

- GPT (Generative Pre-trained Transformer) : 生成式预训练语言模型
- LLaMA (Large Language Model Meta AI) : facebook 大语言模型
    * [Introducing LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
- Alpaca () : 
