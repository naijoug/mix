# LLMs 
> Large Language Models (å¤§è¯­è¨€æ¨¡å‹)

## Reference

- [langchain](https://github.com/hwchase17/langchain)
    > âš¡ Building applications with LLMs through composability âš¡

- [LLaMA](https://github.com/facebookresearch/llama)
    > Inference code for LLaMA models
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
    > Port of Facebook's LLaMA model in C/C++
- [Stanford Alpaca](https//github.com/tatsu-lab/stanford_alpaca)
    > Code and documentation to train Stanford's Alpaca models, and generate the data.

- [guidance](https://github.com/microsoft/guidance)
    > A guidance language for controlling large language models.    
- [GPT4All](https://github.com/nomic-ai/gpt4all) : [Official](https://gpt4all.io)
    > gpt4all: an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue
- [ChatLLaMa](https://github.com/juncongmoo/chatllama)
    > ChatLLaMA ğŸ“¢ Open source implementation for LLaMA-based ChatGPT runnable in a single GPU. 15x faster training process than ChatGPT
- [pyllama](https://github.com/juncongmoo/pyllama)
    > ğŸ¦™ LLaMA - Run LLM in A Single 4GB GPU
- [minichatgpt](https://github.com/juncongmoo/minichatgpt) 
    > ğŸ”¥ To Train ChatGPT In 5 Minutes with ColossalAI
- [privateGPT](https://github.com/imartinez/privateGPT)
    > Interact privately with your documents using the power of GPT, 100% privately, no data leaks
- [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)
    > ä¸­æ–‡LLaMA&Alpacaå¤§è¯­è¨€æ¨¡å‹+æœ¬åœ°CPU/GPUéƒ¨ç½² (Chinese LLaMA & Alpaca LLMs)
- [KoAlpaca](https://github.com/Beomi/KoAlpaca)
    > KoAlpaca: Korean Alpaca Model based on Stanford Alpaca (feat. LLAMA and Polyglot-ko)

## Concept

- GPT (Generative Pre-trained Transformer) : ç”Ÿæˆå¼é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
- LLaMA (Large Language Model Meta AI) : facebook å¤§è¯­è¨€æ¨¡å‹
    * [Introducing LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
- Alpaca () : 
