# LLMs 
> Large Language Models (å¤§è¯­è¨€æ¨¡å‹)

## Reference

- [Open LLMs](https://github.com/eugeneyan/open-llms)
    > ğŸ¤– A list of open LLMs available for commercial use.   

## GPT

- [GPT4All](https://github.com/nomic-ai/gpt4all) : [Official](https://gpt4all.io)
    > gpt4all: an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue
- [minichatgpt](https://github.com/juncongmoo/minichatgpt) 
    > ğŸ”¥ To Train ChatGPT In 5 Minutes with ColossalAI
- [privateGPT](https://github.com/imartinez/privateGPT)
    > Interact privately with your documents using the power of GPT, 100% privately, no data leaks

## LLaMa

- [LLaMA](https://github.com/facebookresearch/llama) : [introducing](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
    > Inference code for LLaMA models
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
    > Port of Facebook's LLaMA model in C/C++
- [ChatLLaMa](https://github.com/juncongmoo/chatllama)
    > ChatLLaMA ğŸ“¢ Open source implementation for LLaMA-based ChatGPT runnable in a single GPU. 15x faster training process than ChatGPT
- [pyllama](https://github.com/juncongmoo/pyllama)
    > ğŸ¦™ LLaMA - Run LLM in A Single 4GB GPU

## Alpaca

- [Stanford Alpaca](https//github.com/tatsu-lab/stanford_alpaca)
    > An Instruction-following LLaMA Model
- [alpaca-lora](https://github.com/tloen/alpaca-lora)
    > Instruct-tune LLaMA on consumer hardware
- [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)
    > ä¸­æ–‡LLaMA&Alpacaå¤§è¯­è¨€æ¨¡å‹+æœ¬åœ°CPU/GPUéƒ¨ç½² (Chinese LLaMA & Alpaca LLMs)
- [KoAlpaca](https://github.com/Beomi/KoAlpaca)
    > KoAlpaca: Korean Alpaca Model based on Stanford Alpaca (feat. LLAMA and Polyglot-ko)

## Vicuna

- [FastChat](https://github.com/lm-sys/FastChat)
    > An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and FastChat-T5.
- [Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna)
    > Chinese-Vicuna: A Chinese Instruction-following LLaMA-based Model â€”â€” ä¸€ä¸ªä¸­æ–‡ä½èµ„æºçš„llama+loraæ–¹æ¡ˆï¼Œç»“æ„å‚è€ƒalpaca
- [WizardVicunaLM](https://github.com/melodysdreamj/WizardVicunaLM)
    > LLM that combines the principles of wizardLM and vicunaLM

## Other

- [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)
    > ChatGLM-6B: An Open Bilingual Dialogue Language Model | å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹
- [Luotuo-Chinese-LLM](https://github.com/LC1332/Luotuo-Chinese-LLM)
    > éª†é©¼(Luotuo): Open Sourced Chinese Language Models
- [Visual OpenLLM](https://github.com/visual-openllm/visual-openllm)
    > something like visual-chatgpt, æ–‡å¿ƒä¸€è¨€çš„å¼€æºç‰ˆ
- [wenda](https://github.com/wenda-LLM/wenda)
    > é—»è¾¾ï¼šä¸€ä¸ªLLMè°ƒç”¨å¹³å°ã€‚ä¸ºå°æ¨¡å‹å¤–æŒ‚çŸ¥è¯†åº“æŸ¥æ‰¾å’Œè®¾è®¡è‡ªåŠ¨æ‰§è¡ŒåŠ¨ä½œï¼Œå®ç°ä¸äºšäºäºå¤§æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›
- [Linly](https://github.com/CVI-SZU/Linly)
    > Chinese-LLaMAåŸºç¡€æ¨¡å‹ï¼›ChatFlowä¸­æ–‡å¯¹è¯æ¨¡å‹ï¼›ä¸­æ–‡OpenLLaMAæ¨¡å‹ï¼›NLPé¢„è®­ç»ƒ/æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†
- [CPM-Bee](https://github.com/OpenBMB/CPM-Bee)
    > ç™¾äº¿å‚æ•°çš„ä¸­è‹±æ–‡åŒè¯­åŸºåº§å¤§æ¨¡å‹
    
## Tutorial

- [åŸºäºæœ¬åœ°çŸ¥è¯†åº“çš„ ChatGLM ç­‰å¤§è¯­è¨€æ¨¡å‹åº”ç”¨å®ç°](https://github.com/imClumsyPanda/langchain-ChatGLM)
- [ä¸€ç§å¹³ä»·çš„chatgptå®ç°æ–¹æ¡ˆ, åŸºäºChatGLM-6B + LoRA](https://github.com/mymusise/ChatGLM-Tuning)
- [2023-05-18 GPTå¤§è¯­è¨€æ¨¡å‹Vicunaæœ¬åœ°åŒ–éƒ¨ç½²å®è·µï¼ˆæ•ˆæœç§’æ€Alpacaï¼‰](https://zhuanlan.zhihu.com/p/630287397)
- [2023-04-22 å¤§æ¨¡å‹ä¹Ÿå†…å·ï¼ŒVicunaè®­ç»ƒåŠæ¨ç†æŒ‡å—ï¼Œæ•ˆæœç¢¾å‹æ–¯å¦ç¦ç¾Šé©¼](https://zhuanlan.zhihu.com/p/624012908)